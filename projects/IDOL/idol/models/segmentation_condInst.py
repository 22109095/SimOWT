# ------------------------------------------------------------------------
# IDOL: In Defense of Online Models for Video Instance Segmentation
# Copyright (c) 2022 ByteDance. All Rights Reserved.
# ------------------------------------------------------------------------
# Modified from SeqFormer (https://github.com/wjf5203/SeqFormer)
# Copyright (c) 2021 ByteDance. All Rights Reserved.
# ------------------------------------------------------------------------
# Modified from Deformable DETR (https://github.com/fundamentalvision/Deformable-DETR)
# Copyright (c) 2020 SenseTime. All Rights Reserved.
# ------------------------------------------------------------------------
# Modified from DETR (https://github.com/facebookresearch/detr)
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
# ------------------------------------------------------------------------

"""
This file provides the definition of the convolutional heads used to predict masks, as well as the losses
"""
import io
import os
import os.path
from collections import defaultdict
from matplotlib.pyplot import box
import copy
import torch
import torch.nn as nn
import torch.nn.functional as F
from PIL import Image


from ..util.misc import NestedTensor, interpolate, nested_tensor_from_tensor_list, inverse_sigmoid
from .pos_neg_select import select_pos_neg

from fvcore.transforms.transform import (
    CropTransform,
    HFlipTransform,
    NoOpTransform,
    Transform,
    TransformList,
)
from detectron2.data.transforms import ResizeTransform, CropTransform
from .deformable_transformer import DeformableTransformerDecoderLayer, DeformableReidHead

class CondInst_segm(nn.Module):
    def __init__(self, detr, rel_coord=True, freeze_detr=False, use_iou_branch=False, reid_depth=False, only_det=False):
        super().__init__()
        if not only_det:
            self.detr = detr
            self.rel_coord = rel_coord

            hidden_dim, nheads = detr.transformer.d_model, detr.transformer.nhead

            self.in_channels = hidden_dim // 32
            self.dynamic_mask_channels = 8
            self.controller_layers = 3
            self.max_insts_num = 100
            self.mask_out_stride = 4
            self.up_rate = 8 // self.mask_out_stride

            # dynamic_mask_head params
            weight_nums, bias_nums = [], []
            for l in range(self.controller_layers):
                if l == 0:
                    if self.rel_coord:
                        weight_nums.append((self.in_channels + 2) * self.dynamic_mask_channels)
                    else:
                        weight_nums.append(self.in_channels * self.dynamic_mask_channels)
                    bias_nums.append(self.dynamic_mask_channels)
                elif l == self.controller_layers - 1:
                    weight_nums.append(self.dynamic_mask_channels * 1)
                    bias_nums.append(1)
                else:
                    weight_nums.append(self.dynamic_mask_channels * self.dynamic_mask_channels)
                    bias_nums.append(self.dynamic_mask_channels)

            self.weight_nums = weight_nums
            self.bias_nums = bias_nums
            self.num_gen_params = sum(weight_nums) + sum(bias_nums)

            self.controller = MLP(hidden_dim, hidden_dim, self.num_gen_params, 3)

            for contr in self.controller.layers:
                nn.init.xavier_uniform_(contr.weight)
                nn.init.zeros_(contr.bias)

            self.mask_head = MaskHeadSmallConv(hidden_dim, None, hidden_dim)

            self.reid_depth = reid_depth
            if reid_depth:
                nheads = 8
                dropout = 0.1
                dim_feedforward = 1024
                dec_n_points = 4
                num_feature_levels = 4
                decoder_layer = DeformableTransformerDecoderLayer(hidden_dim, dim_feedforward,
                dropout, "relu", num_feature_levels, nheads, dec_n_points)
                self.reid_embed_head = nn.ModuleList([
                    DeformableReidHead(hidden_dim, decoder_layer, 2),
                    MLP(hidden_dim, hidden_dim, hidden_dim, 3)]
                )
                a = 1
            else:
                self.reid_embed_head = MLP(hidden_dim, hidden_dim, hidden_dim, 3)  # [ATTN] emb_head  hidden_dim = 256
            self.use_iou_branch = use_iou_branch
            self.a = 0
        else:
            self.detr = detr
            self.rel_coord = rel_coord

            hidden_dim, nheads = detr.transformer.d_model, detr.transformer.nhead

            self.in_channels = hidden_dim // 32
            self.dynamic_mask_channels = 8
            self.controller_layers = 3
            self.max_insts_num = 100
            self.mask_out_stride = 4
            self.up_rate = 8 // self.mask_out_stride

            # dynamic_mask_head params
            weight_nums, bias_nums = [], []
            for l in range(self.controller_layers):
                if l == 0:
                    if self.rel_coord:
                        weight_nums.append((self.in_channels + 2) * self.dynamic_mask_channels)
                    else:
                        weight_nums.append(self.in_channels * self.dynamic_mask_channels)
                    bias_nums.append(self.dynamic_mask_channels)
                elif l == self.controller_layers - 1:
                    weight_nums.append(self.dynamic_mask_channels * 1)
                    bias_nums.append(1)
                else:
                    weight_nums.append(self.dynamic_mask_channels * self.dynamic_mask_channels)
                    bias_nums.append(self.dynamic_mask_channels)

            self.weight_nums = weight_nums
            self.bias_nums = bias_nums
            self.num_gen_params = sum(weight_nums) + sum(bias_nums)

            self.controller = MLP(hidden_dim, hidden_dim, self.num_gen_params, 3)

            for contr in self.controller.layers:
                nn.init.xavier_uniform_(contr.weight)
                nn.init.zeros_(contr.bias)

            self.mask_head = MaskHeadSmallConv(hidden_dim, None, hidden_dim)
            self.use_iou_branch = use_iou_branch
            self.a = 0

    def forward(self, samples, det_targets, ref_targets, file_names, files, criterion, tags, train=False, only_det=False):
        if not only_det:
            image_sizes = samples.image_sizes
            if not isinstance(samples, NestedTensor):
                samples = nested_tensor_from_tensor_list(samples, size_divisibility=32)  # NestedTensor

            features, pos = self.detr.backbone(samples)  #

            srcs = []
            masks = []
            poses = []
            spatial_shapes = []

            for l, feat in enumerate(features[1:]):
                src, mask = feat.decompose() # nestedTensor.decompose
                src_proj_l = self.detr.input_proj[l](src)    # src_proj_l: [N, C, Hi, Wi]
                srcs.append(src_proj_l)
                masks.append(mask)
                poses.append(pos[l+1])
                n, c, h, w = src_proj_l.shape
                spatial_shapes.append((h, w))

            if self.detr.num_feature_levels > (len(features) - 1):
                _len_srcs = len(features) - 1
                for l in range(_len_srcs, self.detr.num_feature_levels):
                    if l == _len_srcs:
                        src = self.detr.input_proj[l](features[-1].tensors)
                    else:
                        src = self.detr.input_proj[l](srcs[-1])
                    m = masks[0]   # [N, H, W]
                    mask = F.interpolate(m[None].float(), size=src.shape[-2:]).to(torch.bool)[0]
                    pos_l = self.detr.backbone[1](NestedTensor(src, mask)).to(src.dtype)
                    srcs.append(src)
                    masks.append(mask)
                    poses.append(pos_l)
                    n, c, h, w = src.shape
                    spatial_shapes.append((h, w))
            query_embeds = None
            if not self.detr.two_stage:
                query_embeds = self.detr.query_embed.weight

            srcs_key = []
            masks_key = []
            poses_key = []

            srcs_reference = []
            masks_reference = []
            poses_reference = []

            bz = samples.tensors.shape[0]//2
            key_ids = list(range(0,bz*2-1,2))
            ref_ids = list(range(1,bz*2,2))

            for n_l in range(self.detr.num_feature_levels ):
                srcs_key.append(srcs[n_l][key_ids])
                srcs_reference.append(srcs[n_l][ref_ids])
                masks_key.append(masks[n_l][key_ids])
                masks_reference.append(masks[n_l][ref_ids])
                poses_key.append(poses[n_l][key_ids])
                poses_reference.append(poses[n_l][ref_ids])
            image_sizes = [image_sizes[_i] for _i in key_ids]
            if self.reid_depth:
                hs, memory, init_reference, inter_references, enc_outputs_class, enc_outputs_coord_unact, src_info_key = self.detr.transformer(
                    srcs_key, masks_key, poses_key, query_embeds, return_src_info=True)
                hs_ref, memory_ref, init_reference_ref, inter_references_ref, _un2, _un3, src_info_ref = self.detr.transformer(
                    srcs_reference,
                    masks_reference,
                    poses_reference,
                    query_embeds, return_src_info=True)
                a = 1
                src_info_key["reference_points"] = inter_references[-1].detach()
                src_info_ref["reference_points"] = inter_references_ref[-1].detach()
                a = 1
            else:
                hs, memory, init_reference, inter_references, enc_outputs_class, enc_outputs_coord_unact = self.detr.transformer(
                    srcs_key, masks_key, poses_key, query_embeds, return_src_info=False)
                hs_ref, memory_ref, init_reference_ref, inter_references_ref, _un2, _un3 = self.detr.transformer(srcs_reference,
                                                                                                                 masks_reference,
                                                                                                                 poses_reference,
                                                                                                                 query_embeds, return_src_info=False)
                src_info_key, src_info_ref = None, None

            outputs = {}
            outputs_classes = []
            outputs_coords = []
            outputs_masks = []
            indices_list = []
            ious_list = []
            if self.use_iou_branch:
                outputs_ious = []
            enc_lay_num = hs.shape[0]

            for lvl in range(enc_lay_num):
                if lvl == 0:
                    reference = init_reference
                else:
                    reference = inter_references[lvl - 1]
                reference = inverse_sigmoid(reference)
                outputs_class = self.detr.class_embed[lvl](hs[lvl])  # [ATTN]
                tmp = self.detr.bbox_embed[lvl](hs[lvl])
                if self.use_iou_branch:
                    pred_iou = self.detr.iou_head[lvl](hs[lvl])
                if reference.shape[-1] == 4:
                    tmp += reference
                else:
                    assert reference.shape[-1] == 2
                    tmp[..., :2] += reference
                outputs_coord = tmp.sigmoid()
                outputs_classes.append(outputs_class)
                outputs_coords.append(outputs_coord)
                if self.use_iou_branch:
                    outputs_ious.append(pred_iou)
                outputs_layer = {'pred_logits': outputs_class, 'pred_boxes': outputs_coord}
                dynamic_mask_head_params = self.controller(hs[lvl])    # [bs, num_quries, num_params]

                # for training & log evaluation loss
                indices, matched_ids, ious = criterion.matcher(outputs_layer, det_targets, file_names)
                indices_list.append(indices)
                ious_list.append(ious)

                reference_points, mask_head_params, num_insts = [], [], []
                for i, indice in enumerate(indices):
                    pred_i, tgt_j = indice
                    # num_insts.append(len(pred_i))
                    num_insts.append(pred_i.sum())
                    mask_head_params.append(dynamic_mask_head_params[i, pred_i].unsqueeze(0))

                    # This is the image size after data augmentation (so as the gt boxes & masks)

                    orig_h, orig_w = image_sizes[i]
                    orig_h = torch.as_tensor(orig_h).to(reference)
                    orig_w = torch.as_tensor(orig_w).to(reference)
                    scale_f = torch.stack([orig_w, orig_h], dim=0)

                    ref_cur_f = reference[i].sigmoid()
                    ref_cur_f = ref_cur_f[..., :2]
                    ref_cur_f = ref_cur_f * scale_f[None, :]
                    reference_points.append(ref_cur_f[pred_i].unsqueeze(0))
                reference_points = torch.cat(reference_points, dim=1)
                mask_head_params = torch.cat(mask_head_params, dim=1)

                # mask prediction
                outputs_layer = self.forward_mask_head_train(outputs_layer, memory, spatial_shapes,
                                                             reference_points, mask_head_params, num_insts)
                outputs_masks.append(outputs_layer['pred_masks'])


            outputs_class = torch.stack(outputs_classes)
            outputs_coord = torch.stack(outputs_coords)
            outputs_mask = outputs_masks


            ref_cls = self.detr.class_embed[-1](hs_ref[-1]).sigmoid()
            contrast_items, matched_reid = select_pos_neg(inter_references_ref[-1], matched_ids, ref_targets, det_targets,
                                            self.reid_embed_head, hs[-1], hs_ref[-1], ref_cls, reid_depth=self.reid_depth, src_info_key=src_info_key, src_info_ref=src_info_ref)
            outputs['pred_logits'] = outputs_class[-1]  # [ATTN]
            outputs['pred_boxes'] = outputs_coord[-1]
            outputs['pred_masks'] = outputs_mask[-1]
            outputs['pred_qd'] = contrast_items
            if self.use_iou_branch:
                outputs_iou = torch.stack(outputs_ious)
                outputs['pred_boxious'] = outputs_iou[-1]

            if self.detr.aux_loss:
                if self.use_iou_branch:
                    outputs['aux_outputs'] = self._set_aux_loss_with_iou(outputs_class, outputs_coord, outputs_mask, outputs_iou)
                else:
                    outputs['aux_outputs'] = self._set_aux_loss(outputs_class, outputs_coord, outputs_mask)
            if self.detr.two_stage:
                enc_outputs_coord = enc_outputs_coord_unact.sigmoid()
                outputs['enc_outputs'] = {'pred_logits': enc_outputs_class, 'pred_boxes': enc_outputs_coord}
            if train:  # [ATTN]
                loss_dict = criterion(outputs, det_targets,ref_targets, indices_list, ious_list,
                                      files, file_names, tags, matched_reid)
            else:
                loss_dict = None

            return outputs, loss_dict
        else:
            image_sizes = samples.image_sizes
            if not isinstance(samples, NestedTensor):
                samples = nested_tensor_from_tensor_list(samples, size_divisibility=32)  # NestedTensor

            features, pos = self.detr.backbone(samples)  #

            srcs = []
            masks = []
            poses = []
            spatial_shapes = []

            for l, feat in enumerate(features[1:]):
                # src: [N, _C, Hi, Wi],
                # mask: [N, Hi, Wi],
                # pos: [N, C, H_p, W_p]
                src, mask = feat.decompose()  # nestedTensor.decompose
                src_proj_l = self.detr.input_proj[l](
                    src)
                srcs.append(src_proj_l)
                masks.append(mask)
                poses.append(pos[l + 1])
                n, c, h, w = src_proj_l.shape
                spatial_shapes.append((h, w))

            if self.detr.num_feature_levels > (len(features) - 1):
                _len_srcs = len(features) - 1
                for l in range(_len_srcs, self.detr.num_feature_levels):
                    if l == _len_srcs:
                        src = self.detr.input_proj[l](features[-1].tensors)
                    else:
                        src = self.detr.input_proj[l](srcs[-1])
                    m = masks[0]  # [N, H, W]
                    mask = F.interpolate(m[None].float(), size=src.shape[-2:]).to(torch.bool)[0]
                    pos_l = self.detr.backbone[1](NestedTensor(src, mask)).to(src.dtype)
                    srcs.append(src)
                    masks.append(mask)
                    poses.append(pos_l)
                    n, c, h, w = src.shape
                    spatial_shapes.append((h, w))
            query_embeds = None
            if not self.detr.two_stage:
                query_embeds = self.detr.query_embed.weight

            srcs_key = []
            masks_key = []
            poses_key = []

            srcs_reference = []
            masks_reference = []
            poses_reference = []

            bz = samples.tensors.shape[0] // 2
            key_ids = list(range(0, bz * 2 - 1, 2))

            for n_l in range(self.detr.num_feature_levels):
                srcs_key.append(srcs[n_l][key_ids])
                masks_key.append(masks[n_l][key_ids])
                poses_key.append(poses[n_l][key_ids])
            image_sizes = [image_sizes[_i] for _i in key_ids]
            hs, memory, init_reference, inter_references, enc_outputs_class, enc_outputs_coord_unact, src_info_key = self.detr.transformer(
                srcs_key, masks_key, poses_key, query_embeds, return_src_info=True)
            outputs = {}
            outputs_classes = []
            outputs_coords = []
            outputs_masks = []
            indices_list = []
            ious_list = []
            if self.use_iou_branch:
                outputs_ious = []
            enc_lay_num = hs.shape[0]

            for lvl in range(enc_lay_num):
                if lvl == 0:
                    reference = init_reference
                else:
                    reference = inter_references[lvl - 1]
                reference = inverse_sigmoid(reference)
                outputs_class = self.detr.class_embed[lvl](hs[lvl])  # [ATTN]
                tmp = self.detr.bbox_embed[lvl](hs[lvl])
                if self.use_iou_branch:
                    pred_iou = self.detr.iou_head[lvl](hs[lvl])
                if reference.shape[-1] == 4:
                    tmp += reference
                else:
                    assert reference.shape[-1] == 2
                    tmp[..., :2] += reference
                outputs_coord = tmp.sigmoid()
                outputs_classes.append(outputs_class)
                outputs_coords.append(outputs_coord)
                if self.use_iou_branch:
                    outputs_ious.append(pred_iou)
                outputs_layer = {'pred_logits': outputs_class, 'pred_boxes': outputs_coord}
                dynamic_mask_head_params = self.controller(hs[lvl])  # [bs, num_quries, num_params]

                # for training & log evaluation loss
                indices, matched_ids, ious = criterion.matcher(outputs_layer, det_targets, file_names)
                indices_list.append(indices)
                ious_list.append(ious)

                reference_points, mask_head_params, num_insts = [], [], []
                for i, indice in enumerate(indices):
                    pred_i, tgt_j = indice
                    # num_insts.append(len(pred_i))
                    num_insts.append(pred_i.sum())
                    mask_head_params.append(dynamic_mask_head_params[i, pred_i].unsqueeze(0))

                    # This is the image size after data augmentation (so as the gt boxes & masks)

                    orig_h, orig_w = image_sizes[i]
                    orig_h = torch.as_tensor(orig_h).to(reference)
                    orig_w = torch.as_tensor(orig_w).to(reference)
                    scale_f = torch.stack([orig_w, orig_h], dim=0)

                    ref_cur_f = reference[i].sigmoid()
                    ref_cur_f = ref_cur_f[..., :2]
                    ref_cur_f = ref_cur_f * scale_f[None, :]
                    reference_points.append(ref_cur_f[pred_i].unsqueeze(0))
                # reference_points: [1, nf,  \sum{selected_insts}, 2]
                # mask_head_params: [1, \sum{selected_insts}, num_params]
                reference_points = torch.cat(reference_points, dim=1)
                mask_head_params = torch.cat(mask_head_params, dim=1)

                # mask prediction
                outputs_layer = self.forward_mask_head_train(outputs_layer, memory, spatial_shapes,
                                                             reference_points, mask_head_params, num_insts)
                outputs_masks.append(outputs_layer['pred_masks'])

            outputs_class = torch.stack(outputs_classes)
            outputs_coord = torch.stack(outputs_coords)
            outputs_mask = outputs_masks

            # ref_cls = self.detr.class_embed[-1](hs_ref[-1]).sigmoid()
            # ref_cls = self.detr.class_embed[5](hs_ref[-1]).sigmoid()

            # det1 = []
            # ref1 = []
            # matched_ids1 = []
            # for i in range(len(tags)):
            #     det1.append(det_targets[i].copy())
            #     ref1.append(ref_targets[i].copy())
            #     matched_ids1.append(matched_ids[i].clone())
            #
            # for count in range(len(tags)):
            #     if len(tags[count])==0:
            #         continue
            #     if tags[count][0]!=2:
            #         truth_list = []
            #         for i in range(len(tags[count])):
            #             if tags[count][i]==0:
            #                 truth_list.append(True)
            #             else:
            #                 truth_list.append(False)
            #         det1[count]['labels'] = det1[count]['labels'][truth_list]
            #         det1[count]['boxes'] = det1[count]['boxes'][truth_list]
            #         det1[count]['masks'] = det1[count]['masks'][truth_list]
            #         det1[count]['inst_id'] = det1[count]['inst_id'][truth_list]
            #         det1[count]['valid'] = det1[count]['valid'][truth_list]
            #         det1[count]['longscore'] = det1[count]['longscore'][truth_list]
            #
            #         ref1[count]['labels'] = ref1[count]['labels'][truth_list]
            #         ref1[count]['boxes'] = ref1[count]['boxes'][truth_list]
            #         ref1[count]['masks'] = ref1[count]['masks'][truth_list]
            #         ref1[count]['inst_id'] = ref1[count]['inst_id'][truth_list]
            #         ref1[count]['valid'] = ref1[count]['valid'][truth_list]
            #         ref1[count]['longscore'] = ref1[count]['longscore'][truth_list]
            #         matched_ids1[count] = matched_ids1[count][truth_list]
            # a = 1
            # contrast_items = select_pos_neg(inter_references_ref[-1], matched_ids1, ref1, det1,
            #                                 self.reid_embed_head, hs[-1], hs_ref[-1], ref_cls)
            a = 1
            # contrast_items = select_pos_neg(inter_references_ref[-1], matched_ids, ref_targets, det_targets,
            #                                 self.reid_embed_head, hs[-1], hs_ref[-1], ref_cls)
            a = 1
            # contrast_items = select_pos_neg(inter_references_ref[-1], matched_ids, ref_targets, det_targets,
            #                                 self.reid_embed_head, hs[-1], hs_ref[-1], ref_cls,
            #                                 reid_depth=self.reid_depth, src_info_key=src_info_key,
            #                                 src_info_ref=src_info_ref)

            outputs['pred_logits'] = outputs_class[-1]  # [ATTN]
            outputs['pred_boxes'] = outputs_coord[-1]
            outputs['pred_masks'] = outputs_mask[-1]
            # outputs['pred_qd'] = contrast_items
            if self.use_iou_branch:
                outputs_iou = torch.stack(outputs_ious)
                outputs['pred_boxious'] = outputs_iou[-1]

            if self.detr.aux_loss:
                if self.use_iou_branch:
                    outputs['aux_outputs'] = self._set_aux_loss_with_iou(outputs_class, outputs_coord, outputs_mask,
                                                                         outputs_iou)
                else:
                    outputs['aux_outputs'] = self._set_aux_loss(outputs_class, outputs_coord, outputs_mask)
            if self.detr.two_stage:
                enc_outputs_coord = enc_outputs_coord_unact.sigmoid()
                outputs['enc_outputs'] = {'pred_logits': enc_outputs_class, 'pred_boxes': enc_outputs_coord}
            if train:  # [ATTN]
                loss_dict = criterion(outputs, det_targets, ref_targets, indices_list, ious_list, files, file_names, tags, only_det=only_det)
            else:
                loss_dict = None

            return outputs, loss_dict

    def vis_feature_backbone(self, feature, sub_root, b):
        a = 1
        device = feature[0].tensors.device
        root = '/ssd1/wby_workspace/VNext/demo_img/pix/'
        import matplotlib.pyplot as plt
        for i in range(len(feature)):
            _, C, h, w = feature[i].tensors.shape
            vis_feature = torch.zeros(h, w).to(device)
            feature_sub = feature[i].tensors[0]
            for feat in feature_sub:
                a = 1
                vis_feature = vis_feature + feat
            vis_feature = (vis_feature/C*255).int().cpu().numpy()
            plt.imshow(vis_feature, cmap=plt.cm.jet)
            if not os.path.exists(root+sub_root):
                os.makedirs(root+sub_root)
            plt.savefig(root+sub_root+'{}_{}.jpg'.format(str(b).zfill(5), str(i).zfill(2)))
            # plt.show()
            a = 1
        return

    def vis_feature_encoder(self, feature, sub_root, b):
        a = 1
        device = feature[0].device
        root = '/ssd1/wby_workspace/VNext/demo_img/pix/'
        import matplotlib.pyplot as plt
        for i in range(len(feature)):
            _, C, h, w = feature[i].shape
            vis_feature = torch.zeros(h, w).to(device)
            feature_sub = feature[i][0]
            for feat in feature_sub:
                a = 1
                vis_feature = vis_feature + feat
            vis_feature = (vis_feature / C * 255).int().cpu().numpy()
            plt.imshow(vis_feature, cmap=plt.cm.jet)
            if not os.path.exists(root+sub_root):
                os.makedirs(root+sub_root)
            plt.savefig(root+sub_root+'{}_{}.jpg'.format(str(b).zfill(5), str(i).zfill(2)))
            # plt.show()
            # plt.savefig(root + sub_root + '{}.jpg'.format(str(i).zfill(2)))
            a = 1
        return

    def vis_feature_hm(self, feature, sub_root, b):
        a = 1
        device = feature[0].device
        root = '/ssd1/wby_workspace/VNext/demo_img/pix/'
        import matplotlib.pyplot as plt
        for i in range(len(feature)):
            _, C, h, w = feature[i].shape
            vis_feature = torch.zeros(h, w).to(device)
            feature_sub = feature[i][0]
            for feat in feature_sub:
                a = 1
                vis_feature = vis_feature + feat
            vis_feature = (vis_feature / C * 255).int().cpu().numpy()
            plt.imshow(vis_feature, cmap=plt.cm.jet)
            if not os.path.exists(root+sub_root):
                os.makedirs(root+sub_root)
            plt.savefig(root+sub_root+'{}_{}.jpg'.format(str(b).zfill(5), str(i).zfill(2)))
            # plt.show()
            # plt.savefig(root + sub_root + '{}.jpg'.format(str(i).zfill(2)))
            a = 1
        return

    def inference_forward(self, samples, size_divisib=32, only_det=False):  # [ATTN] coco inference
        image_sizes = samples.image_sizes
        if not isinstance(samples, NestedTensor):
            samples = nested_tensor_from_tensor_list(samples, size_divisibility=size_divisib)
    
        features, pos = self.detr.backbone(samples)
        
        srcs = []
        masks = []
        poses = []
        spatial_shapes = []

        for l, feat in enumerate(features[1:]):
            # src: [N, _C, Hi, Wi],
            # mask: [N, Hi, Wi],
            # pos: [N, C, H_p, W_p]
            src, mask = feat.decompose() 
            src_proj_l = self.detr.input_proj[l](src)    # src_proj_l: [N, C, Hi, Wi]
            srcs.append(src_proj_l)
            masks.append(mask)
            poses.append(pos[l+1])
            n, c, h, w = src_proj_l.shape
            spatial_shapes.append((h, w))


        if self.detr.num_feature_levels > (len(features) - 1):
            _len_srcs = len(features) - 1
            for l in range(_len_srcs, self.detr.num_feature_levels):
                if l == _len_srcs:
                    src = self.detr.input_proj[l](features[-1].tensors)
                else:
                    src = self.detr.input_proj[l](srcs[-1])
                m = masks[0]   # [N, H, W]
                mask = F.interpolate(m[None].float(), size=src.shape[-2:]).to(torch.bool)[0]
                pos_l = self.detr.backbone[1](NestedTensor(src, mask)).to(src.dtype)
                srcs.append(src)
                masks.append(mask)
                poses.append(pos_l)
                n, c, h, w = src.shape
                spatial_shapes.append((h, w))
        features2vis = srcs

        # query_embeds = self.detr.query_embed.weight
        if not self.detr.two_stage:
            query_embeds = self.detr.query_embed.weight

            # hs, memory, init_reference, inter_references, inter_samples, enc_outputs_class, _ = \
            #     self.detr.transformer(srcs, masks, poses, query_embeds)
            hs, memory, init_reference, inter_references, enc_outputs_class, enc_outputs_coord_unact, src_info = self.detr.transformer(
                srcs, masks, poses, query_embeds, return_src_info=True)
            if not only_det:
                if self.reid_depth:
                    src_info["reference_points"] = inter_references[-1].detach()
        else:
            query_embeds = None
            hs, memory, init_reference, inter_references, enc_outputs_class, enc_outputs_coord_unact = \
                    self.detr.transformer(srcs, masks, poses, query_embeds)

        outputs = {}
        if self.use_iou_branch:
            outputs_ious = []

        reference = inter_references[-1 - 1]
        reference = inverse_sigmoid(reference)
        outputs_class = self.detr.class_embed[-1](hs[-1])  # [ATTN]
        # outputs_class = self.detr.class_embed[5](hs[-1])  # [ATTN]
        tmp = self.detr.bbox_embed[-1](hs[-1])
        # tmp = self.detr.bbox_embed[5](hs[-1])
        if self.use_iou_branch:
            pred_iou = self.detr.iou_head[-1](hs[-1])
            # pred_iou = self.detr.iou_head[5](hs[-1])
        if reference.shape[-1] == 4:
            tmp += reference
        else:
            assert reference.shape[-1] == 2
            tmp[..., :2] += reference
        outputs_coord = tmp.sigmoid()
        if self.use_iou_branch:
            outputs_ious.append(pred_iou)
            outputs_iou = torch.stack(outputs_ious)
            outputs['pred_boxious'] = outputs_iou[-1]
        outputs['pred_logits'] = outputs_class
        outputs['pred_boxes'] = outputs_coord
        if not only_det:
            if self.reid_depth:
                inst_embed = self.reid_embed_head[1](self.reid_embed_head[0](
                    hs[-1], src_info["reference_points"], src_info["src"],
                    src_info["src_spatial_shapes"], src_info["src_level_start_index"],
                    src_info["src_valid_ratios"], src_info["src_padding_mask"], reid_depth=self.reid_depth))
            else:
                inst_embed = self.reid_embed_head(hs[-1])
            outputs['pred_inst_embed'] = inst_embed


        outputs['reference_points'] = inter_references[-2, :, :, :2]
        dynamic_mask_head_params = self.controller(hs[-1])
        bs, num_queries, _ = dynamic_mask_head_params.shape
        num_insts = [num_queries for i in range(bs)]
        reference_points = []
        for i, image_size_i in enumerate(image_sizes):
            orig_h, orig_w = image_size_i
            orig_h = torch.as_tensor(orig_h).to(outputs['reference_points'][i])
            orig_w = torch.as_tensor(orig_w).to(outputs['reference_points'][i])
            scale_f = torch.stack([orig_w, orig_h], dim=0)
            ref_cur_f = outputs['reference_points'][i] * scale_f[None, :]
            reference_points.append(ref_cur_f.unsqueeze(0))
        # reference_points: [1, N * num_queries, 2]
        # mask_head_params: [1, N * num_queries, num_params]
        reference_points = torch.cat(reference_points, dim=1)
        mask_head_params = dynamic_mask_head_params.reshape(1, -1, dynamic_mask_head_params.shape[-1])
        outputs = self.forward_mask_head_train(outputs, memory, spatial_shapes, 
                                                reference_points, mask_head_params, num_insts)
        # outputs['pred_masks']: [bs, num_queries, num_frames, H/4, W/4]
        outputs['pred_masks'] = torch.cat(outputs['pred_masks'], dim=0)
     

        # return outputs, features
        return outputs, features2vis


    def forward_mask_head_train(self, outputs, feats, spatial_shapes, reference_points, mask_head_params, num_insts):
        bs, _, c = feats.shape
        # nq = mask_head_params.shape[1]

        # encod_feat_l: num_layers x [bs, C, num_frames, hi, wi]
        encod_feat_l = []
        spatial_indx = 0
        for feat_l in range(self.detr.num_feature_levels - 1):
            h, w = spatial_shapes[feat_l]
            mem_l = feats[:, spatial_indx: spatial_indx + 1 * h * w, :].reshape(bs, 1, h, w, c).permute(0,4,1,2,3)
            encod_feat_l.append(mem_l)
            spatial_indx += 1 * h * w
        
        pred_masks = []
        for iframe in range(1):
            encod_feat_f = []
            for lvl in range(self.detr.num_feature_levels - 1):
                encod_feat_f.append(encod_feat_l[lvl][:, :, iframe, :, :]) # [bs, C, hi, wi]
            
            decod_feat_f = self.mask_head(encod_feat_f, fpns=None)
          
            ######### conv ##########
            mask_logits = self.dynamic_mask_with_coords(decod_feat_f, reference_points, mask_head_params, 
                                                        num_insts=num_insts,
                                                        mask_feat_stride=8,
                                                        rel_coord=self.rel_coord )
            # mask_logits: [1, num_queries_all, H/4, W/4]

            # mask_f = mask_logits.unsqueeze(2).reshape(bs, nq, 1, decod_feat_f.shape[-2], decod_feat_f.shape[-1])  # [bs, selected_queries, 1, H/4, W/4]
            mask_f = []
            inst_st = 0
            for num_inst in num_insts:
                mask_f.append(mask_logits[:, inst_st: inst_st + num_inst, :, :].unsqueeze(2))
                inst_st += num_inst

            pred_masks.append(mask_f)  
        
        output_pred_masks = []
        for i, num_inst in enumerate(num_insts):
            out_masks_b = [m[i] for m in pred_masks]
            output_pred_masks.append(torch.cat(out_masks_b, dim=2))
        
        outputs['pred_masks'] = output_pred_masks
        return outputs


    def mask_heads_forward(self, features, weights, biases, num_insts):
        '''
        :param features
        :param weights: [w0, w1, ...]
        :param bias: [b0, b1, ...]
        :return:
        '''
        assert features.dim() == 4
        n_layers = len(weights)
        x = features
        for i, (w, b) in enumerate(zip(weights, biases)):
            x = F.conv2d(
                x, w, bias=b,
                stride=1, padding=0,
                groups=num_insts
            )
            if i < n_layers - 1:
                x = F.relu(x)
        return x


    def dynamic_mask_with_coords(self, mask_feats, reference_points, mask_head_params, num_insts, 
                                 mask_feat_stride, rel_coord=True):
        device = mask_feats.device

        N, in_channels, H, W = mask_feats.size()
        num_insts_all = reference_points.shape[1]

        locations = compute_locations(
            mask_feats.size(2), mask_feats.size(3), 
            device=device, stride=mask_feat_stride)
        # locations: [H*W, 2]
        
        if rel_coord:
            instance_locations = reference_points
            relative_coords = instance_locations.reshape(1, num_insts_all, 1, 1, 2) - locations.reshape(1, 1, H, W, 2)
            relative_coords = relative_coords.float()
            relative_coords = relative_coords.permute(0, 1, 4, 2, 3).flatten(-2, -1)
            mask_head_inputs = []
            inst_st = 0
            for i, num_inst in enumerate(num_insts):
                # [1, num_queries * (C/32+2), H/8 * W/8]
                relative_coords_b = relative_coords[:, inst_st: inst_st + num_inst, :, :]
                mask_feats_b = mask_feats[i].reshape(1, in_channels, H * W).unsqueeze(1).repeat(1, num_inst, 1, 1)
                mask_head_b = torch.cat([relative_coords_b, mask_feats_b], dim=2)

                mask_head_inputs.append(mask_head_b)
                inst_st += num_inst

        else:
            mask_head_inputs = []
            inst_st = 0
            for i, num_inst in enumerate(num_insts):
                mask_head_b = mask_feats[i].reshape(1, in_channels, H * W).unsqueeze(1).repeat(1, num_inst, 1, 1)
                mask_head_b = mask_head_b.reshape(1, -1, H, W)
                mask_head_inputs.append(mask_head_b)
        
        # mask_head_inputs: [1, \sum{num_queries * (C/32+2)}, H/8, W/8]
        mask_head_inputs = torch.cat(mask_head_inputs, dim=1)
        mask_head_inputs = mask_head_inputs.reshape(1, -1, H, W)

        # mask_head_params: [num_insts_all, num_params]
        mask_head_params = torch.flatten(mask_head_params, 0, 1)
       
        if num_insts_all != 0:
            weights, biases = parse_dynamic_params(
                mask_head_params, self.dynamic_mask_channels,
                self.weight_nums, self.bias_nums
            )

            mask_logits = self.mask_heads_forward(mask_head_inputs, weights, biases, mask_head_params.shape[0])
        else:
            mask_logits = mask_head_inputs
            return mask_logits
        # mask_logits: [1, num_insts_all, H/8, W/8]
        mask_logits = mask_logits.reshape(-1, 1, H, W)

        # upsample predicted masks
        assert mask_feat_stride >= self.mask_out_stride
        assert mask_feat_stride % self.mask_out_stride == 0

        mask_logits = aligned_bilinear(mask_logits, int(mask_feat_stride / self.mask_out_stride))

        mask_logits = mask_logits.reshape(1, -1, mask_logits.shape[-2], mask_logits.shape[-1])
        # mask_logits: [1, num_insts_all, H/4, W/4]

        return mask_logits


    def _set_aux_loss(self, outputs_class, outputs_coord, outputs_mask):
        # this is a workaround to make torchscript happy, as torchscript
        # doesn't support dictionary with non-homogeneous values, such
        # as a dict having both a Tensor and a list.
        return [{'pred_logits': a, 'pred_boxes': b, 'pred_masks': c}
                for a, b, c in zip(outputs_class[:-1], outputs_coord[:-1], outputs_mask[:-1])]

    def _set_aux_loss_with_iou(self, outputs_class, outputs_coord, outputs_mask, outputs_iou):
        # this is a workaround to make torchscript happy, as torchscript
        # doesn't support dictionary with non-homogeneous values, such
        # as a dict having both a Tensor and a list.
        return [{'pred_logits': a, 'pred_boxes': b, 'pred_masks': c, 'pred_boxious': d}
                for a, b, c, d in zip(outputs_class[:-1], outputs_coord[:-1], outputs_mask[:-1], outputs_iou[:-1])]

class MaskHeadSmallConv(nn.Module):
    """
    Simple convolutional head, using group norm.
    Upsampling is done using a FPN approach
    """

    def __init__(self, dim, fpn_dims, context_dim):
        super().__init__()

        # inter_dims = [dim, context_dim // 2, context_dim // 4, context_dim // 8, context_dim // 16, context_dim // 64]
        inter_dims = [dim, context_dim, context_dim, context_dim, context_dim, context_dim]

        # used after upsampling to reduce dimention of fused features!
        self.lay1 = torch.nn.Conv2d(dim, dim//4, 3, padding=1)
        self.lay2 = torch.nn.Conv2d(dim//4, dim//32, 3, padding=1)
        self.lay3 = torch.nn.Conv2d(inter_dims[1], inter_dims[2], 3, padding=1)
        self.lay4 = torch.nn.Conv2d(inter_dims[2], inter_dims[3], 3, padding=1)
        self.dcn = torch.nn.Conv2d(inter_dims[3], inter_dims[4], 3, padding=1)
        self.dim = dim

        if fpn_dims != None:
            self.adapter1 = torch.nn.Conv2d(fpn_dims[0], inter_dims[1], 1)
            self.adapter2 = torch.nn.Conv2d(fpn_dims[1], inter_dims[2], 1)
            self.adapter3 = torch.nn.Conv2d(fpn_dims[2], inter_dims[3], 1)

        for name, m in self.named_modules():
            if name == "conv_offset":
                nn.init.constant_(m.weight, 0)
                nn.init.constant_(m.bias, 0)
            else:
                if isinstance(m, nn.Conv2d):
                    nn.init.kaiming_uniform_(m.weight, a=1)
                    nn.init.constant_(m.bias, 0)


    def forward(self, x, fpns):

        if fpns != None:
            cur_fpn = self.adapter1(fpns[0])
            if cur_fpn.size(0) != x[-1].size(0):
                cur_fpn = _expand(cur_fpn, x[-1].size(0) // cur_fpn.size(0))
            fused_x = (cur_fpn + x[-1]) / 2
        else:
            fused_x = x[-1]
        fused_x = self.lay3(fused_x)
        fused_x = F.relu(fused_x)

        if fpns != None:
            cur_fpn = self.adapter2(fpns[1])
            if cur_fpn.size(0) != x[-2].size(0):
                cur_fpn = _expand(cur_fpn, x[-2].size(0) // cur_fpn.size(0))
            fused_x = (cur_fpn + x[-2]) / 2 + F.interpolate(fused_x, size=cur_fpn.shape[-2:], mode="nearest")
        else:
            fused_x = x[-2] + F.interpolate(fused_x, size=x[-2].shape[-2:], mode="nearest")
        fused_x = self.lay4(fused_x)
        fused_x = F.relu(fused_x)

        if fpns != None:
            cur_fpn = self.adapter3(fpns[2])
            if cur_fpn.size(0) != x[-3].size(0):
                cur_fpn = _expand(cur_fpn, x[-3].size(0) // cur_fpn.size(0))
            fused_x = (cur_fpn + x[-3]) / 2 + F.interpolate(fused_x, size=cur_fpn.shape[-2:], mode="nearest")
        else:
            fused_x = x[-3] + F.interpolate(fused_x, size=x[-3].shape[-2:], mode="nearest")
        fused_x = self.dcn(fused_x)
        fused_x = F.relu(fused_x)
        fused_x = self.lay1(fused_x)
        fused_x = F.relu(fused_x)
        fused_x = self.lay2(fused_x)
        fused_x = F.relu(fused_x)

        return fused_x



def _expand(tensor, length: int):
    return tensor.unsqueeze(1).repeat(1, int(length), 1, 1, 1).flatten(0, 1)


class MHAttentionMap(nn.Module):
    """This is a 2D attention module, which only returns the attention softmax (no multiplication by value)"""

    def __init__(self, query_dim, hidden_dim, num_heads, dropout=0, bias=True):
        super().__init__()
        self.num_heads = num_heads
        self.hidden_dim = hidden_dim
        self.dropout = nn.Dropout(dropout)

        self.q_linear = nn.Linear(query_dim, hidden_dim, bias=bias)
        self.k_linear = nn.Linear(query_dim, hidden_dim, bias=bias)

        nn.init.zeros_(self.k_linear.bias)
        nn.init.zeros_(self.q_linear.bias)
        nn.init.xavier_uniform_(self.k_linear.weight)
        nn.init.xavier_uniform_(self.q_linear.weight)
        self.normalize_fact = float(hidden_dim / self.num_heads) ** -0.5

    def forward(self, q, k, mask=None):
        q = self.q_linear(q)
        k = F.conv2d(k, self.k_linear.weight.unsqueeze(-1).unsqueeze(-1), self.k_linear.bias)
        qh = q.view(q.shape[0], q.shape[1], self.num_heads, self.hidden_dim // self.num_heads)
        kh = k.view(k.shape[0], self.num_heads, self.hidden_dim // self.num_heads, k.shape[-2], k.shape[-1])
        weights = torch.einsum("bqnc,bnchw->bqnhw", qh * self.normalize_fact, kh)

        if mask is not None:
            weights.masked_fill_(mask.unsqueeze(1).unsqueeze(1), float("-inf"))
        weights = F.softmax(weights.flatten(2), dim=-1).view_as(weights)
        weights = self.dropout(weights)
        return weights


def parse_dynamic_params(params, channels, weight_nums, bias_nums):
    assert params.dim() == 2
    assert len(weight_nums) == len(bias_nums)
    assert params.size(1) == sum(weight_nums) + sum(bias_nums)

    num_insts = params.size(0)
    num_layers = len(weight_nums)

    params_splits = list(torch.split_with_sizes(params, weight_nums + bias_nums, dim=1))

    weight_splits = params_splits[:num_layers]
    bias_splits = params_splits[num_layers:]

    for l in range(num_layers):
        if l < num_layers - 1:
            # out_channels x in_channels x 1 x 1
            weight_splits[l] = weight_splits[l].reshape(num_insts * channels, -1, 1, 1)
            bias_splits[l] = bias_splits[l].reshape(num_insts * channels)
        else:
            # out_channels x in_channels x 1 x 1
            weight_splits[l] = weight_splits[l].reshape(num_insts * 1, -1, 1, 1)
            bias_splits[l] = bias_splits[l].reshape(num_insts)

    return weight_splits, bias_splits


def aligned_bilinear(tensor, factor):
    assert tensor.dim() == 4
    assert factor >= 1
    assert int(factor) == factor

    if factor == 1:
        return tensor

    h, w = tensor.size()[2:]
    tensor = F.pad(tensor, pad=(0, 1, 0, 1), mode="replicate")
    oh = factor * h + 1
    ow = factor * w + 1
    tensor = F.interpolate(
        tensor, size=(oh, ow),
        mode='bilinear',
        align_corners=True
    )
    tensor = F.pad(
        tensor, pad=(factor // 2, 0, factor // 2, 0),
        mode="replicate"
    )

    return tensor[:, :, :oh - 1, :ow - 1]


def compute_locations(h, w, device, stride=1):
    shifts_x = torch.arange(
        0, w * stride, step=stride,
        dtype=torch.float32, device=device)

    shifts_y = torch.arange(
        0, h * stride, step=stride,
        dtype=torch.float32, device=device)

    shift_y, shift_x = torch.meshgrid(shifts_y, shifts_x)
    shift_x = shift_x.reshape(-1)
    shift_y = shift_y.reshape(-1)
    locations = torch.stack((shift_x, shift_y), dim=1) + stride // 2
    return locations


def dice_loss(inputs, targets, num_boxes):
    """
    Compute the DICE loss, similar to generalized IOU for masks
    Args:
        inputs: A float tensor of arbitrary shape.
                The predictions for each example.
        targets: A float tensor with the same shape as inputs. Stores the binary
                 classification label for each element in inputs
                (0 for the negative class and 1 for the positive class).
    """
    inputs = inputs.sigmoid()
    inputs = inputs.flatten(1)
    numerator = 2 * (inputs * targets).sum(1)
    denominator = inputs.sum(-1) + targets.sum(-1)
    loss = 1 - (numerator + 1) / (denominator + 1)
    return loss.sum() / num_boxes


def sigmoid_focal_loss(inputs, targets, num_boxes, alpha: float = 0.25, gamma: float = 2):
    """
    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.
    Args:
        inputs: A float tensor of arbitrary shape.
                The predictions for each example.
        targets: A float tensor with the same shape as inputs. Stores the binary
                 classification label for each element in inputs
                (0 for the negative class and 1 for the positive class).
        alpha: (optional) Weighting factor in range (0,1) to balance
                positive vs negative examples. Default = -1 (no weighting).
        gamma: Exponent of the modulating factor (1 - p_t) to
               balance easy vs hard examples.
    Returns:
        Loss tensor
    """
    prob = inputs.sigmoid()
    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction="none")
    p_t = prob * targets + (1 - prob) * (1 - targets)
    loss = ce_loss * ((1 - p_t) ** gamma)

    if alpha >= 0:
        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)
        loss = alpha_t * loss

    return loss.mean(1).sum() / num_boxes



class MLP(nn.Module):
    """ Very simple multi-layer perceptron (also called FFN)"""

    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):
        super().__init__()
        self.num_layers = num_layers
        h = [hidden_dim] * (num_layers - 1)
        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))

    def forward(self, x):
        for i, layer in enumerate(self.layers):
            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)
        return x


from detectron2.structures import Instances,ROIMasks
def segmentation_postprocess1(results, output_height, output_width, mask_threshold=0.5):
    """
    For instance segmentation whose masks are size of batched output,
    not regional sizes as from R-CNN based predictor.
    """
    scale_x, scale_y = (float(output_width) / results.image_size[1], float(output_height) / results.image_size[0])
    results = Instances((output_height, output_width), **results.get_fields())

    if results.has("pred_boxes"):
        output_boxes = results.pred_boxes
    elif results.has("proposal_boxes"):
        output_boxes = results.proposal_boxes

    output_boxes.scale(scale_x, scale_y)
    output_boxes.clip(results.image_size)

    if results.has("pred_masks"):
        results.pred_masks = interpolate(
            results.pred_masks.float().unsqueeze(1), size=(output_height, output_width),
            mode='bilinear'
        ).squeeze(1) > 0.5

    results = results[output_boxes.nonempty()]

    return results



def segmentation_postprocess(
    results: Instances, output_height: int, output_width: int, mask_threshold: float = 0.5
    ):

    if isinstance(output_width, torch.Tensor):
        # This shape might (but not necessarily) be tensors during tracing.
        # Converts integer tensors to float temporaries to ensure true
        # division is performed when computing scale_x and scale_y.
        output_width_tmp = output_width.float()
        output_height_tmp = output_height.float()
        new_size = torch.stack([output_height, output_width])
    else:
        new_size = (output_height, output_width)
        output_width_tmp = output_width
        output_height_tmp = output_height

    scale_x, scale_y = (
        output_width_tmp / results.image_size[1],
        output_height_tmp / results.image_size[0],
    )
    # print('fields = {}'.format(results.get_fields()))
    results = Instances(new_size, **results.get_fields())

    if results.has("pred_boxes"):
        output_boxes = results.pred_boxes
    elif results.has("proposal_boxes"):
        output_boxes = results.proposal_boxes
    else:
        output_boxes = None
    assert output_boxes is not None, "Predictions must contain boxes!"

    output_boxes.scale(scale_x, scale_y)
    output_boxes.clip(results.image_size)

    results = results[output_boxes.nonempty()]

    if results.has("pred_masks"):
        mask = F.interpolate(results.pred_masks.float(), size=(output_height, output_width), mode='nearest')
        mask = mask.squeeze(1).byte()
        results.pred_masks = mask

    return results
